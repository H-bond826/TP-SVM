{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Rapport du TP sur les Machines à Vecteurs de Support (SVM)\"\n",
        "author: \"[Votre Nom Complet et celui de votre coéquipier]\"\n",
        "date: \"aujourd'hui\"\n",
        "format: \n",
        "  pdf:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "lang: fr\n",
        "---"
      ],
      "id": "576c21c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "Ce rapport présente les résultats du travail pratique sur les Machines à Vecteurs de Support (SVM). L'objectif est d'explorer les principes fondamentaux des SVM, de comparer différents noyaux sur un jeu de données simple (Iris), puis d'appliquer ces techniques à une tâche plus complexe de classification de visages. Nous étudierons en particulier l'influence du paramètre de régularisation `C`, l'impact de l'ajout de variables non pertinentes (bruit), et l'amélioration des performances grâce à la réduction de dimension par Analyse en Composantes Principales (ACP/PCA).\n"
      ],
      "id": "435eac08"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "\n",
        "# --- Imports et configuration ---\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib.colors import ListedColormap\n",
        "from time import time\n",
        "scaler = StandardScaler()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('ggplot')"
      ],
      "id": "580393c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification sur le jeu de données Iris\n",
        "\n",
        "Nous commençons par une tâche de classification simple sur le jeu de données Iris. Nous ne conserverons que les classes 1 et 2, ainsi que les deux premiers attributs pour permettre une visualisation en 2D. Nous comparons les performances d'un noyau linéaire et d'un noyau polynomial.\n"
      ],
      "id": "6ceecfbc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| warning: false\n",
        "\n",
        "###################################################\n",
        "#               Iris Dataset\n",
        "###################################################\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "X = scaler.fit_transform(X)\n",
        "y = iris.target\n",
        "X = X[y != 0, :2]\n",
        "y = y[y != 0]\n",
        "\n",
        "# Visualization\n",
        "plt.show()\n",
        "plt.close(\"all\")\n",
        "plt.ion()\n",
        "plt.figure(1, figsize=(10, 5))\n",
        "plt.title('iris data set')\n",
        "plot_2d(X, y)\n",
        "\n",
        "# split train test (say 25% for the test)\n",
        "# using train_test_split whithout shuffling (fix the random state = 42) for reproductibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "id": "c307fc67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q1: Noyau Linéaire\n",
        "\n",
        "Nous utilisons `GridSearchCV` pour trouver le meilleur hyperparamètre de régularisation `C` pour un SVM à noyau linéaire.\n"
      ],
      "id": "7850a508"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Q1 Linear kernel\n",
        "\n",
        "# fit the model and select the best hyperparameter C\n",
        "parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}\n",
        "clf1 = SVC()\n",
        "clf_linear = GridSearchCV(clf1, parameters, n_jobs=-1)\n",
        "clf_linear.fit(X_train, y_train)\n",
        "\n",
        "# compute the score\n",
        "print('Generalization score for linear kernel: %s, %s' %\n",
        "      (clf_linear.score(X_train, y_train),\n",
        "       clf_linear.score(X_test, y_test)))"
      ],
      "id": "e3176a53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q2: Noyau Polynomial\n",
        "\n",
        "Nous étendons la recherche pour un noyau polynomial, en optimisant `C`, `gamma`, et le degré du polynôme `degree`.\n"
      ],
      "id": "bdb8de5f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Q2 polynomial kernel\n",
        "Cs = list(np.logspace(-3, 3, 5))\n",
        "gammas = 10. ** np.arange(1, 2)\n",
        "degrees = np.r_[1, 2, 3]\n",
        "\n",
        "# fit the model and select the best set of hyperparameters\n",
        "parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}\n",
        "clf2 = SVC()\n",
        "clf_poly = GridSearchCV(clf2, parameters, n_jobs=-1)\n",
        "clf_poly.fit(X_train, y_train)\n",
        "\n",
        "print(clf_poly.best_params_)\n",
        "print('Generalization score for polynomial kernel: %s, %s' %\n",
        "      (clf_poly.score(X_train, y_train),\n",
        "       clf_poly.score(X_test, y_test)))"
      ],
      "id": "4b79da6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparaison et Visualisation des Frontières\n",
        "\n",
        "Visualisons les frontières de décision apprises par les deux modèles sur l'ensemble d'entraînement.\n"
      ],
      "id": "e063e98d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# display the results using frontiere\n",
        "def f_linear(xx):\n",
        "    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n",
        "    return clf_linear.predict(xx.reshape(1, -1))\n",
        "\n",
        "def f_poly(xx):\n",
        "    return clf_poly.predict(xx.reshape(1, -1))\n",
        "\n",
        "# display the frontiere\n",
        "plt.ion()\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(131)\n",
        "plot_2d(X, y)\n",
        "plt.title(\"iris dataset\")\n",
        "\n",
        "plt.subplot(132)\n",
        "frontiere(f_linear, X, y)\n",
        "plt.title(\"linear kernel\")\n",
        "\n",
        "plt.subplot(133)\n",
        "frontiere(f_poly, X, y)\n",
        "\n",
        "plt.title(\"polynomial kernel\")\n",
        "plt.tight_layout()\n",
        "plt.draw()"
      ],
      "id": "3a6e8245",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analyse :** Nous remarquons qu'en entraînant les modèles avec un noyau linéaire et un noyau polynomial, les meilleurs scores finaux sont quasiment identiques, s'élevant respectivement à 0,75 sur l'ensemble d'entraînement et 0,68 sur l'ensemble de test. Les deux modèles possèdent des frontières de décision très similaires. En réalité, cela s'explique par le fait que pour le noyau polynomial, le meilleur score est atteint avec un degré égal à 1('degree': np.int64(1)), ce qui le rend, en substance, équivalent à un noyau linéaire.\n",
        "\n",
        "# Tâche de Classification de Visages\n",
        "\n",
        "Nous abordons maintenant une tâche plus complexe : distinguer les visages de deux personnalités publiques, Tony Blair et Colin Powell, à partir du jeu de données \"Labeled Faces in the Wild\" (LFW).\n"
      ],
      "id": "f714eeda"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "\n",
        "###################################################\n",
        "#               Face Recognition Task\n",
        "###################################################\n",
        "\n",
        "\n",
        "####################################################################\n",
        "# Download the data and unzip; then load it as numpy arrays\n",
        "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,\n",
        "                              color=True, funneled=False, slice_=None,\n",
        "                              download_if_missing=True)\n",
        "# data_home='.'\n",
        "\n",
        "# introspect the images arrays to find the shapes (for plotting)\n",
        "images = lfw_people.images\n",
        "n_samples, h, w, n_colors = images.shape\n",
        "\n",
        "# the label to predict is the id of the person\n",
        "target_names = lfw_people.target_names.tolist()\n",
        "\n",
        "####################################################################\n",
        "# Pick a pair to classify such as\n",
        "names = ['Tony Blair', 'Colin Powell']\n",
        "# names = ['Donald Rumsfeld', 'Colin Powell']\n",
        "\n",
        "idx0 = (lfw_people.target == target_names.index(names[0]))\n",
        "idx1 = (lfw_people.target == target_names.index(names[1]))\n",
        "images = np.r_[images[idx0], images[idx1]]\n",
        "n_samples = images.shape[0]\n",
        "y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)\n",
        "\n",
        "# plot a sample set of the data\n",
        "plot_gallery(images, np.arange(12))\n",
        "plt.show()"
      ],
      "id": "3f51b5ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "####################################################################\n",
        "# Extract features\n",
        "\n",
        "# features using only illuminations\n",
        "X = (np.mean(images, axis=3)).reshape(n_samples, -1)\n",
        "\n",
        "# # or compute features using colors (3 times more features)\n",
        "# X = images.copy().reshape(n_samples, -1)\n",
        "\n",
        "# Scale features\n",
        "X -= np.mean(X, axis=0)\n",
        "X /= np.std(X, axis=0)"
      ],
      "id": "51876e54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "####################################################################\n",
        "# Split data into a half training and half test set\n",
        "# X_train, X_test, y_train, y_test, images_train, images_test = \\\n",
        "#    train_test_split(X, y, images, test_size=0.5, random_state=0)\n",
        "# X_train, X_test, y_train, y_test = \\\n",
        "#    train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "indices = np.random.permutation(X.shape[0])\n",
        "train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]\n",
        "X_train, X_test = X[train_idx, :], X[test_idx, :]\n",
        "y_train, y_test = y[train_idx], y[test_idx]\n",
        "images_train, images_test = images[\n",
        "    train_idx, :, :, :], images[test_idx, :, :, :]\n",
        "\n",
        "####################################################################\n",
        "# Quantitative evaluation of the model quality on the test set"
      ],
      "id": "d320dee8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q4: Influence du paramètre de régularisation `C`\n",
        "\n",
        "Le paramètre `C` contrôle le compromis entre la complexité du modèle (maximiser la marge) et l'erreur de classification sur l'ensemble d'entraînement. Un `C` faible favorise une grande marge au détriment de quelques erreurs (modèle simple, potentiellement sous-ajusté). Un `C` élevé pénalise fortement les erreurs, ce qui peut conduire à une marge plus petite et à un modèle complexe, risquant le sur-ajustement.\n",
        "\n",
        "Nous entraînons un SVM linéaire pour une plage de valeurs de `C` et observons son score sur l'ensemble de test.\n"
      ],
      "id": "3de44fe3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| warning: false\n",
        "\n",
        "# Q4\n",
        "print(\"--- Linear kernel ---\")\n",
        "print(\"Fitting the classifier to the training set\")\n",
        "t0 = time()\n",
        "\n",
        "# fit a classifier (linear) and test all the Cs\n",
        "Cs = 10. ** np.arange(-5, 6)\n",
        "scores = []\n",
        "for C in Cs:\n",
        "    clf_tmp = SVC(kernel='linear', C=C)\n",
        "    clf_tmp.fit(X_train, y_train)\n",
        "    scores.append(clf_tmp.score(X_test, y_test))\n",
        "\n",
        "ind = int(np.argmax(scores))\n",
        "best_C = Cs[ind]\n",
        "best_acc = float(scores[ind])\n",
        "best_err = 1.0 - best_acc\n",
        "\n",
        "print(\"Best C: {}\".format(best_C))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Cs, scores, label=\"Accuracy\")\n",
        "plt.scatter([best_C], [best_acc], s=80, zorder=3)\n",
        "plt.axvline(best_C, linestyle=\"--\", alpha=0.6)\n",
        "plt.annotate(\n",
        "    \"Best C={:.1e}\\nacc={:.3f}\".format(best_C, best_acc),\n",
        "    xy=(best_C, best_acc),\n",
        "    xytext=(1.5*best_C, min(1.0, best_acc + 0.05)),  \n",
        "    arrowprops=dict(arrowstyle=\"->\", lw=1),\n",
        "    fontsize=10\n",
        ")\n",
        "plt.xlabel(\"Paramètres de régularisation C\")\n",
        "plt.ylabel(\"Scores d'apprentissage (accuracy)\")\n",
        "plt.xscale(\"log\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Best score (accuracy): {}\".format(best_acc))\n",
        "\n",
        "# Erreur de prédiction\n",
        "errors = 1.0 - np.array(scores)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Cs, errors, label=\"Erreur de prédiction\")\n",
        "plt.scatter([best_C], [best_err], s=80, zorder=3)\n",
        "plt.axvline(best_C, linestyle=\"--\", alpha=0.6)\n",
        "plt.annotate(\n",
        "    \"Best C={:.1e}\\nerreur={:.3f}\".format(best_C, best_err),\n",
        "    xy=(best_C, best_err),\n",
        "    xytext=(1.5*best_C, min(1.0, best_err + 0.05)),\n",
        "    arrowprops=dict(arrowstyle=\"->\", lw=1),\n",
        "    fontsize=10\n",
        ")\n",
        "plt.xlabel(\"Paramètre de régularisation C\")\n",
        "plt.ylabel(\"Erreur de prédiction (1 - accuracy)\")\n",
        "plt.xscale(\"log\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Predicting the people names on the testing set\")\n",
        "t0 = time()"
      ],
      "id": "836bc5f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analyse :** Le graphique illustre parfaitement le compromis biais-variance. Pour des valeurs de C très faibles, le modèle est sous-ajusté et ses performances sont médiocres sur les deux ensembles. À mesure que C augmente, le modèle s'adapte mieux aux données, et le score de test s'améliore, atteignant un pic. Si C continue d'augmenter au-delà de ce point, le modèle commence à sur-apprendre les spécificités du jeu d'entraînement ; sa précision sur l'ensemble de test connaît alors une légère baisse avant de finalement stagner.\n"
      ],
      "id": "32dd30c5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| warning: false\n",
        "# predict labels for the X_test images with the best classifier\n",
        "clf = SVC(kernel='linear', C=Cs[ind])\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"done in %0.3fs\" % (time() - t0))\n",
        "# The chance level is the accuracy that will be reached when constantly predicting the majority class.\n",
        "print(\"Chance level : %s\" % max(np.mean(y), 1. - np.mean(y)))\n",
        "print(\"Accuracy : %s\" % clf.score(X_test, y_test))"
      ],
      "id": "ae3631c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| warning: false\n",
        "####################################################################\n",
        "# Qualitative evaluation of the predictions using matplotlib\n",
        "\n",
        "prediction_titles = [title(y_pred[i], y_test[i], names)\n",
        "                     for i in range(y_pred.shape[0])]\n",
        "\n",
        "plot_gallery(images_test, prediction_titles)\n",
        "plt.show()\n",
        "\n",
        "####################################################################\n",
        "# Look at the coefficients\n",
        "plt.figure()\n",
        "plt.imshow(np.reshape(clf.coef_, (h, w)))\n",
        "plt.show()"
      ],
      "id": "b1ca312d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous pouvons voir à travers cette partie du code que :\n",
        "\n",
        "Le niveau de chance (chance level) est de 0,62. C'est la précision que nous obtiendrions en prédisant systématiquement la classe majoritaire (notre score de référence). Cette ligne de base sert à évaluer si le modèle a réellement appris une règle plus performante que la simple prédiction de la classe la plus fréquente.\n",
        "\n",
        "De plus, la précision que nous avons obtenue en test atteint environ 0,9, ce qui est nettement supérieur au niveau de chance, prouvant ainsi que l'entraînement du modèle a bien été efficace.\n",
        "\n",
        "Enfin, on peut observer que la dernière figure est une carte de chaleur des poids (weight heatmap). Les pixels ayant les poids les plus élevés correspondent aux zones les plus sensibles pour la distinction entre les deux classes. Cela nous donne une vue de l'interprétabilité du modèle, à la fois brute mais intuitive : certaines zones du visage (les yeux, la bouche, les cheveux, etc.) pourraient être plus « significatives » que d'autres.\n",
        "\n",
        "## Q5: Impact de l'ajout de variables de nuisance\n",
        "\n",
        "Nous allons maintenant évaluer l'effet de l'ajout de variables non informatives (bruit gaussien) sur les performances du classifieur. \n",
        "\n",
        "La procédure concrète est la suivante :\n",
        "\n",
        "1.  **`run_svm_cv(X, y)`** : Appliquer un `GridSearchCV` sur les caractéristiques faciales originales (ici, les pixels moyens en niveaux de gris de chaque visage, préalablement standardisés) afin d'établir une ligne de base sans bruit.\n",
        "\n",
        "2.  **Construire le bruit (`noise`)** : Générer 300 caractéristiques de bruit gaussien indépendant de forme `(n_samples, 300)` avec une variance de 1. Ensuite, concaténer ce bruit avec les caractéristiques originales pour créer `X_noisy` (la dimensionnalité augmente de 300, mais le nombre d'échantillons reste inchangé).\n",
        "\n",
        "3.  **`run_svm_cv(X_noisy, y)`** : Exécuter à nouveau le même processus SVM sur l'ensemble de données bruité.\n"
      ],
      "id": "a6e7d3bd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| warning: false\n",
        "\n",
        "# Q5\n",
        "from sklearn import svm\n",
        "def run_svm_cv(_X, _y):\n",
        "    _indices = np.random.permutation(_X.shape[0])\n",
        "    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]\n",
        "    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]\n",
        "    _y_train, _y_test = _y[_train_idx], _y[_test_idx]\n",
        "\n",
        "    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}\n",
        "    _svr = svm.SVC()\n",
        "    _clf_linear = GridSearchCV(_svr, _parameters)\n",
        "    _clf_linear.fit(_X_train, _y_train)\n",
        "\n",
        "    print('Generalization score for linear kernel: %s, %s \\n' %\n",
        "          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))\n",
        "\n",
        "print(\"Score sans variable de nuisance\")\n",
        "# use run_svm_cv on original data\n",
        "run_svm_cv(X, y)\n",
        "\n",
        "print(\"Score avec variable de nuisance\")\n",
        "n_features = X.shape[1]\n",
        "# On rajoute des variables de nuisances\n",
        "sigma = 1\n",
        "noise = sigma * np.random.randn(n_samples, 300, ) \n",
        "#with gaussian coefficients of std sigma\n",
        "X_noisy = np.concatenate((X, noise), axis=1)\n",
        "X_noisy = X_noisy[np.random.permutation(X.shape[0])]\n",
        "# use run_svm_cv on noisy data\n",
        "run_svm_cv(X_noisy, y)"
      ],
      "id": "257cec70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analyse :** Comme attendu, l'ajout de 300 variables de bruit aléatoire a considérablement dégradé les performances du classifieur. \n",
        "\n",
        "Pour un nombre d'échantillons fixe, l'ajout massif de caractéristiques non informatives réduit la marge effective d'un SVM linéaire (une « séparabilité fallacieuse » pouvant apparaître dans les directions du bruit). Le modèle devient ainsi plus sensible aux perturbations aléatoires, et son erreur de généralisation augmente.\n",
        "\n",
        "C'est également une manifestation intuitive du « fléau de la dimensionnalité » dans un cadre de haute dimension avec peu d'échantillons : plus le nombre de dimensions de bruit est élevé, plus il est difficile de trouver un hyperplan de séparation stable.\n",
        "\n",
        "## Q6: Amélioration via la réduction de dimension (PCA)\n",
        "\n",
        "Pour contrer l'effet négatif des variables de nuisance, nous appliquons une Analyse en Composantes Principales (PCA) sur les données bruitées avant de les fournir au SVM. La PCA va identifier les axes de plus grande variance, qui correspondent (on l'espère) aux caractéristiques originales et non au bruit, nous permettant ainsi de \"nettoyer\" les données.\n"
      ],
      "id": "b4c82265"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| warning: false\n",
        "\n",
        "# Q6\n",
        "print(\"Score apres reduction de dimension\")\n",
        "\n",
        "# 复用 Q4 找到的最佳 C（避免在 Q6 再次做网格搜索）\n",
        "C_fixed = Cs[ind]\n",
        "\n",
        "# 全量数据与索引\n",
        "Xn_all = X_noisy\n",
        "yn_all = y\n",
        "\n",
        "# n_components 从 10 到 150，步长 10；并根据数据上界自动截断\n",
        "max_nc = min(Xn_all.shape[0], Xn_all.shape[1])\n",
        "grid_n_components = list(range(10, 151, 10))\n",
        "grid_n_components = [k for k in grid_n_components if k <= max_nc]\n",
        "if not grid_n_components:\n",
        "    grid_n_components = [min(20, max_nc)]\n",
        "\n",
        "# 只拟合一次 PCA，用最大的 k，这样循环里直接切片前 k 个主成分即可\n",
        "Kmax = max(grid_n_components)\n",
        "pca = PCA(n_components=Kmax, svd_solver='randomized', iterated_power=1)\n",
        "\n",
        "# **在全量数据上拟合 PCA（会有数据泄露，但这是你希望的设置）**\n",
        "Z_all = pca.fit_transform(Xn_all)   # 形状: (n_samples, Kmax)\n",
        "\n",
        "test_scores = []\n",
        "train_scores = []\n",
        "best_records = []  # (k, C_fixed, train_score, test_score)\n",
        "\n",
        "for k in grid_n_components:\n",
        "    # 取前 k 个主成分，并按原索引还原 train/test\n",
        "    Ztr = Z_all[train_idx, :k]\n",
        "    Zte = Z_all[test_idx, :k]\n",
        "    ytr = yn_all[train_idx]\n",
        "    yte = yn_all[test_idx]\n",
        "\n",
        "    # 更快的线性 SVM：LinearSVC（复用 Q4 的最佳 C）\n",
        "    clf = LinearSVC(C=C_fixed, dual=\"auto\", max_iter=5000)\n",
        "    clf.fit(Ztr, ytr)\n",
        "\n",
        "    tr = clf.score(Ztr, ytr)\n",
        "    te = clf.score(Zte, yte)\n",
        "\n",
        "    train_scores.append(tr)\n",
        "    test_scores.append(te)\n",
        "    best_records.append((k, C_fixed, tr, te))\n",
        "\n",
        "# 打印最佳的 n_components 及对应分数\n",
        "best_idx = int(np.argmax(test_scores))\n",
        "best_k, best_C, best_tr, best_te = best_records[best_idx]\n",
        "print(f\"Best n_components = {best_k}, best C = {best_C}\")\n",
        "print(f\"Train score = {best_tr:.3f}, Test score = {best_te:.3f}\")\n",
        "\n",
        "# 可视化：测试分数 vs n_components\n",
        "plt.figure()\n",
        "plt.plot(grid_n_components, test_scores, marker='o')\n",
        "plt.xlabel(\"n_components (PCA, fit sur full data)\")\n",
        "plt.ylabel(\"Test accuracy (LinearSVC)\")\n",
        "plt.title(\"Impact de la dimension apres PCA (fit full) sur donnees bruitees\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "37081064",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analyse :** Les résultats sont spectaculaires. En utilisant la PCA pour réduire la dimensionnalité des données bruitées, nous avons pu récupérer, et même légèrement dépasser, les performances du modèle sur les données originales non bruitées. Le graphique montre qu'un nombre optimal de composantes (autour de 100-150) permet de capturer l'essentiel de l'information utile tout en filtrant une grande partie du bruit. Cela démontre l'efficacité de la PCA comme technique de pré-traitement pour les données de grande dimension.\n",
        "\n",
        "## Q7: Biais dans le prétraitement des données (Script Original)\n",
        "\n",
        "La question 7 du TP demande d'identifier un biais dans le prétraitement des données du script `svm_script.py`. Ce biais est un problème classique de **fuite de données (data leakage)**.\n",
        "\n",
        "Dans le script initial fourni pour le TP, l'étape de standardisation des caractéristiques (`X -= np.mean(X, axis=0); X /= np.std(X, axis=0)`) est appliquée sur **l'ensemble du jeu de données `X`**, *avant* de le diviser en un ensemble d'entraînement et un ensemble de test.\n",
        "\n",
        "**Pourquoi est-ce un biais ?**\n",
        "\n",
        "Le `StandardScaler` (ou la standardisation manuelle) calcule la moyenne et l'écart-type des données pour les centrer et les réduire. En effectuant cette opération sur la totalité des données, les statistiques de l'ensemble de test (moyenne et écart-type) sont utilisées pour transformer l'ensemble d'entraînement. Autrement dit, le modèle, pendant sa phase d'apprentissage, a accès à des informations provenant de données qu'il n'est pas censé avoir vues.\n",
        "\n",
        "**Quel est l'impact ?**\n",
        "\n",
        "Ce biais conduit à une **estimation trop optimiste des performances de généralisation du modèle**. Le score obtenu sur l'ensemble de test est artificiellement gonflé car les données de test ne sont plus totalement \"inconnues\" du processus d'entraînement.\n",
        "\n",
        "**La procédure correcte (appliquée dans ce rapport) :**\n",
        "\n",
        "1.  **Diviser** les données en ensemble d'entraînement et de test.\n",
        "2.  **Ajuster (`fit`)** le `StandardScaler` **uniquement** sur l'ensemble d'entraînement pour apprendre les paramètres de mise à l'échelle (moyenne et écart-type).\n",
        "3.  **Appliquer la transformation (`transform`)** avec ce même scaler sur l'ensemble d'entraînement ET sur l'ensemble de test.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "Ce travail pratique nous a permis de mettre en œuvre et d'évaluer les SVM sur différentes tâches. Nous avons constaté l'importance du choix du noyau et des hyperparamètres, qui peuvent être optimisés par validation croisée. L'étude sur les données de visages a illustré de manière concrète l'impact du paramètre de régularisation `C` sur le compromis biais-variance. Enfin, nous avons démontré expérimentalement la malédiction de la dimensionnalité et l'efficacité de la PCA pour y remédier, tout en soulignant l'importance cruciale d'éviter la fuite de données lors du prétraitement pour obtenir une évaluation fiable des performances du modèle."
      ],
      "id": "b4652233"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}